# üìä Multi-Source Data Ingestion System

## Overview

The data ingestion system automatically collects Fortnite competitive data from multiple sources, normalizes it into a unified schema, and feeds it to the AI assistant for intelligent querying.

## Data Sources

### 1. Epic CMS (Official) üéÆ
**URL:** `https://fortnitecontent-website-prod07.ol.epicgames.com/content/api/pages/fortnite-game`

**Collects:**
- Battle Royale news and announcements
- Tournament information
- Emergency notices
- Season updates

**Authentication:** None required  
**Rate Limits:** None  
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Current Data:** 189 records

### 2. Fortnite-API.com üîÆ
**URL:** `https://fortnite-api.com/v2`

**Endpoints:**
- `/news` - Latest news items
- `/shop/br` - Item shop (requires API key)
- `/tournaments` - Tournament info

**Authentication:** Optional API key  
**Rate Limits:** Generous  
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê

**Current Data:** 5 news items

### 3. News RSS Feeds üì∞
**Sources:**
- Fortnite Insider (`fortniteinsider.com/feed/`)
- ShiinaBR (`shiinabr.com/feed/`) - Currently unavailable
- HYPEX (`hypex.gg/feed/`) - Currently unavailable

**Collects:**
- Latest news articles
- Leaks and datamines
- Patch notes coverage

**Authentication:** None  
**Rate Limits:** None  
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê

**Current Data:** 10 articles

### 4. Twitter (Competitive) üì±
**Accounts:** osirion_gg, KinchAnalytics, FNcompReport

**Collects:**
- Real-time competitive updates
- Player statistics
- Meta analysis
- Weapon changes

**Authentication:** Bearer token  
**Rate Limits:** 100 posts/month (Free tier)  
**Reliability:** ‚≠ê‚≠ê‚≠ê

**Current Data:** 3 tweets (rate limited)

### 5. Reddit ü§ñ
**Subreddits:** r/FortniteBR, r/FortniteCompetitive, r/FortniteLeaks

**Collects:**
- Community discussions
- Leaks and theories
- Bug reports
- Suggestions

**Authentication:** Client ID + Secret  
**Rate Limits:** 60 requests/minute  
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê

**Current Data:** 0 (not configured)

## Unified Schema

All data normalized to:

```typescript
interface FortniteRecord {
  id: string;                    // Unique identifier
  source: DataSource;            // Source type
  author?: string;               // Author/creator
  title?: string;                // Title (if applicable)
  content: string;               // Main content
  created_at: string;            // ISO timestamp
  url?: string;                  // Source URL
  tags?: string[];               // Categorization tags
  metadata?: Record<string, any>; // Source-specific data
}
```

## Ingestion Process

### 1. Collection
Each source collector runs in parallel:
- Fetches data from its source
- Converts to FortniteRecord format
- Extracts relevant tags
- Returns array of records

### 2. Normalization
- Validates required fields
- Normalizes dates to ISO format
- Cleans content (removes HTML, etc.)
- Extracts tags and metadata

### 3. Deduplication
- Checks record IDs against existing data
- Skips duplicates
- Counts new vs. existing

### 4. Pruning
- Keeps newest 20,000 records
- Sorts by created_at descending
- Removes oldest entries

### 5. Storage
- Saves to `data/ingestion/records.json`
- Creates `data/ingestion/latest.json` (100 newest)
- Logs to `data/ingestion/ingestion.log`

## Scheduling

**Default:** Every 10 minutes (`*/10 * * * *`)

**Run Modes:**

```bash
# One-time ingestion
npm run ingest:once

# Scheduled (every 10 min)
npm start
```

**Cron Configuration:**
Edit `packages/data-ingestion/src/config.ts`:
```typescript
cronSchedule: '*/30 * * * *', // Every 30 minutes
```

## Tag System

Records are automatically tagged for semantic search:

**Tournament Tags:**
- `tournament`, `competitive`, `fncs`, `eval`, `vcc`, `solo`, `duo`, `squad`, `finals`

**Content Tags:**
- `update`, `patch`, `nerf`, `buff`, `weapon`, `meta`

**Community Tags:**
- `leak`, `suggestion`, `bug`, `news`

**Source Tags:**
- `official`, `api`, `reddit`, `news`

## API Integration

### GET /api/data

**Query Parameters:**
- `limit` - Max records (default: 100)
- `source` - Filter by source
- `tag` - Filter by tag

**Examples:**
```bash
# Get all data
GET /api/data

# Get Epic official content
GET /api/data?source=epic&limit=20

# Get tournament info
GET /api/data?tag=tournament

# Get latest news
GET /api/data?source=news&limit=10
```

**Response:**
```json
{
  "data": [...],
  "total": 204,
  "stats": {
    "totalRecords": 204,
    "recordsBySource": {
      "epic": 189,
      "news": 10,
      "fortnite-api": 5
    },
    "lastIngestion": "2025-11-05T01:35:57.247Z"
  }
}
```

## AI Integration

The AI assistant automatically loads ingested data:

```typescript
// packages/ai-assistant/src/data-loader.ts
export async function loadFromIngestion(): Promise<FortniteDataRecord[]> {
  const store = await fs.readJSON(INGESTION_FILE);
  return store.records;
}
```

When you run:
```bash
cd packages/ai-assistant
npm run ingest
```

It will:
1. Load all 204 records from data ingestion
2. Generate embeddings for each
3. Upload to Pinecone (or store in-memory)
4. Make them searchable via `/api/chat`

## Performance

**Ingestion Time:**
- Epic CMS: ~2-3 seconds
- Fortnite-API: ~1-2 seconds
- News RSS: ~2-4 seconds per feed
- Reddit: ~3-5 seconds (when configured)
- **Total:** ~10-20 seconds per run

**Storage:**
- 204 records ‚âà 500KB
- 20,000 records ‚âà 5-10MB
- Logs ‚âà 1MB/month

**Memory:**
- Peak during ingestion: ~100MB
- Idle: ~20MB

## Error Handling

Each source fails independently:
```
‚úÖ Epic CMS successful ‚Üí 189 records
‚úÖ News RSS successful ‚Üí 10 records
‚ùå Reddit failed (no credentials) ‚Üí 0 records
```

**Total Result:** 199 records still collected!

## Logging

**Console Output:**
```
üöÄ Starting data ingestion...
üì° Epic CMS:
  ‚úÖ Collected 189 records
üì° News RSS:
  ‚úÖ Total news records: 10
üìä Ingestion Summary:
  ‚úÖ Successful sources: 4/5
  üÜï New records added: 199
```

**Log File:** `data/ingestion/ingestion.log` (NDJSON format)

```json
{"timestamp":"2025-11-05T01:35:57Z","results":[...],"summary":{"totalNew":199}}
```

## Maintenance

### View Current Data
```bash
# Total records
cat data/ingestion/records.json | jq '.stats.totalRecords'

# By source
cat data/ingestion/records.json | jq '.stats.recordsBySource'

# Latest 5
cat data/ingestion/records.json | jq '.records[:5] | .[] | {source, title}'
```

### Force Re-ingestion
```bash
# Delete existing data
rm data/ingestion/records.json

# Run fresh ingestion
cd packages/data-ingestion
npm run ingest:once
```

### View Logs
```bash
tail -f data/ingestion/ingestion.log | jq '.'
```

## Adding New Sources

### 1. Create Source Module
```typescript
// packages/data-ingestion/src/sources/my-source.ts
import { FortniteRecord } from '../types';

export async function collectMySourceData(): Promise<FortniteRecord[]> {
  const records: FortniteRecord[] = [];
  
  // Your fetch logic here
  
  return records;
}
```

### 2. Add to Orchestrator
```typescript
// packages/data-ingestion/src/index.ts
import { collectMySourceData } from './sources/my-source';

const sources = [
  // ...existing
  { name: 'My Source', fn: collectMySourceData, source: 'my-source' },
];
```

### 3. Update Types
```typescript
// packages/data-ingestion/src/types.ts
export type DataSource = 'twitter' | 'epic' | 'reddit' | 'news' | 'my-source';
```

## Troubleshooting

### "No records found"
Run ingestion first:
```bash
cd packages/data-ingestion
npm run ingest:once
```

### "Source failed"
Check logs for specific error:
```bash
tail data/ingestion/ingestion.log
```

### "Reddit not working"
Add credentials to `.env`:
```env
REDDIT_CLIENT_ID=your_id
REDDIT_SECRET=your_secret
```

### "Too many requests (429)"
Increase cron interval in `config.ts`

## Best Practices

1. **Run ingestion regularly** - Every 10-30 minutes
2. **Monitor logs** - Check for failed sources
3. **Prune old data** - System auto-prunes at 20,000
4. **Tag consistently** - Add relevant tags for better search
5. **Handle errors** - Each source fails independently

## Success Metrics

Current system is collecting:
- ‚úÖ 189 official Epic records
- ‚úÖ 10 news articles
- ‚úÖ 5 Fortnite-API updates
- ‚úÖ 3 competitive tweets
- ‚úÖ 207 total records for AI to use

**Your AI now has rich, multi-source data to answer questions!** üéâ

---

**For support, check:** `START-HERE.md` or `QUICK-FIX.md`

